{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, data_folder, transform=None):\n",
    "        self.data_path = os.path.join(root_dir,data_folder)\n",
    "        self.data = os.listdir(self.data_path)\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data[index]\n",
    "        img_path = os.path.join(self.data_path,img_name)\n",
    "        image = io.imread(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image,img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class addGaussian:\n",
    "    \n",
    "    def __init__(self, std_range):\n",
    "        self.std_range = std_range\n",
    "\n",
    "    def __call__(self,img):\n",
    "        minval,maxval = self.std_range\n",
    "        noise_img = img.astype(np.float)\n",
    "        stddev = np.random.uniform(minval, maxval)\n",
    "        noise = np.random.randn(*img.shape) * stddev\n",
    "        noise_img += noise\n",
    "        noise_img = np.clip(noise_img, 0, 255).astype(np.uint8)\n",
    "        return noise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 1\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dateset = ImageNetDataset('/home/turing/Documents/BE/','data/', \n",
    "                               transform=transforms.Compose([\n",
    "                                   addGaussian((0,50)),\n",
    "                                   Rescale((256,256)),\n",
    "                                   transforms.ToTensor()\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageNetDataset('/home/turing/Documents/BE/','data/', \n",
    "                               transform=transforms.Compose([\n",
    "                                   addGaussian((0,50)),\n",
    "                                   Rescale((256,256)),\n",
    "                                   transforms.ToTensor()\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainLoader(batch_size):\n",
    "    train_loader = DataLoader(train_dateset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "    return(train_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestLoader(batch_size):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "    \n",
    "    return(test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            # first and second layer.\n",
    "            # second layer\n",
    "            nn.Conv2d(3,48,3),\n",
    "            nn.LeakyReLU(True),            \n",
    "            nn.Conv2d(48,48,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # third conv\n",
    "            nn.Conv2d(48,48,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # fourth convolve\n",
    "            nn.Conv2d(48,48,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            # fifth convolve\n",
    "            nn.Conv2d(48,48,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(48,48,3),\n",
    "            nn.LeakyReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(96,96,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(96,64,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64,32,3),\n",
    "            nn.LeakyReLU(True),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(32,3,3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.MSELoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    train_loader = getTrainLoader(batch_size)\n",
    "    \n",
    "    test_loader = getTestLoader(batch_size)\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for inputs, labels in zip(train_loader, test_loader):\n",
    "            \n",
    "            \n",
    "            print(inputs[0].shape)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs[0].float())\n",
    "            print(outputs[0].shape)\n",
    "            loss_size = loss(outputs, labels[0].float())\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss_size.data[0]\n",
    "            \n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "trainNet(net, batch_size=batch_size, n_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
